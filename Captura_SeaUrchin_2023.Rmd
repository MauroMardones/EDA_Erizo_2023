---
title: "Pesquería Erizo X y XI Regiones "
subtitle: "Análisis Exploratorio Bitácora Seguimiento Bentónico 1996-2023"
author: "Mauricio Mardones. Inv. Depto Ev. Recursos. IFOP"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
toc: true
toc-title: "Tabla de Contenidos"
format:
  html:
    theme: cosmo
    fontsize: 0.9em
    linestretch: 1.7
    html-math-method: katex
    self-contained: true
    embed-resources: true
    code-tools: true
#bibliography: biblio.bib
#csl: apa.csl
#link-citations: yes
#linkcolor: blue
---


# Analisis exploratorio de datos de captura de erizo de las X y XI Regiones 2023

En este reporte tiene como objetivo documentar los pasos y criterios establecidos para el analisis exploratorio de los datos de bitacoras del monitoreo de capturas de la pesquería de erizo del sur de Chile.

La finalidad de este trabajo es en primera instancia, dismuniur datos dummies y finalizar con la obtencion  de una señal de abundancia para alimentar el modelo de evaluación de stock, que en este caso es la Captura por Unidad de Esfuerzo CPUE (kg/buzo*hora) estandarizada. Esto tiene como objetivo dejar una base limpia para hacer estandarizaciones de rendimiento de pesca y busqueda del indice de cada una de las zonas evaluadas


Lo primero es usar los datos crudos (raw data) y explorar diferentes variables en tablas y graficos, y así establecer calaramente los criterios usados en filtros y ponderaciones.

la identificación de los distintos indices, patrones y parámetros de la información analizada, se construyen los templates de datos para ser usados en la evaluación de stock implementada en ADMB y sus salidas leídas en un archivo autocontenido .RMD llamado *"Informe_Estatus_Erizo_96_22"* y *"Informe_Estatus_Erizo_60_22"*


### Set de trabajo y librerías utilizadas

```{r setup}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      fig.align="center",
                      fig.path="Figuras/")
require("knitr")
```



Cargo librerias

```{r warning=F, message=F, error=F}
library(GGally)
library(knitr)
library(tidyverse)
library(patchwork)
library(marmap)
library(ggplot2)
library(mapproj)
library(maps)
library(raster)
library(dbplyr)
library(knitr)
library(ggrepel)
library(sf)
library(readxl)
#library(ggOceanMapsData) # Base de costa y profundidad
#library(ggOceanMaps)
library(ggthemes) # nuevos fondos
library(gridExtra)
library(grid)
library(ggthemes)
library(ggalt)
library(here)
library(see)
library(egg)
```


Para el analisis, debemos tener los siguientes archivos de trabajo;

- data cruda de la bitacora de monitoreo de las capturas de erizo.
- Un maestro de procedencias para el cruce de coordenadas geograficas.

Ahora leemos las bases de datos previas.  

```{r}

df96_22 <- read.csv("Capt_1996_2022.csv", sep=",", header = T)
pro <- read.csv2("M_Procedencias.csv", header =T, sep=";")
proifop <- read.csv2("M_Procedencias_Poligonos.csv", header =T, sep=",")

```

Ahora leo los datos del 2023 y Selecciono las columnas relativas a `df96_22`

Identifico la estructura y nombres de ambos `df`

```{r}
df23 <- read_excel("BD_Erizo_2023.xlsx", 
    sheet = "Capturas")
# debo asignar georeferencias
df23b<- df23 %>% 
  mutate(CAPTURA_2=CAPTURA_1) %>% 
  dplyr::select(c("PROCED", "REGION",
           "PUERTO",
           "ANO_REG",
           "MES_REG",
           "DIA_REG",
           "FUNCION",
           "EMBARC",  
           "CAPTURA_1",
           "UNIDAD_1",
           "CAPTURA_2",
            "UNIDAD_2",
           "DESTINO",
           "ANO_ZAR", 
           "MES_ZAR",
           "DIA_ZAR",
           "HOR_ZAR"  ,
           "ANO_ARR"  ,
           "MES_ARR",
           "DIA_ARR",
           "HOR_ARR",
           "PROF",    
           "HOR_BUC",
           "N_BUZOS"))

df96_22b<- df96_22 %>% 
  dplyr::select(-("X"))
```

Uno las bases

```{r}
df96_23 <- rbind(df96_22b, df23b)
```

Ahora llamo al archivo llamado "pro", que son las procedencias con las cuales haremos el cruce para dejar la base con sus georeferencias para plotear los datos en mapas. Luego generamos el cruce de la base junto a las georeferencias

```{r echo=T, include=FALSE}
geocru<- subset(proifop, select=c(4,5,19,20, 21))
colnames(geocru)<- c("PROCED", "POLIGONO","LATITUD", "LONGITUD", "POLIGONO_IFOP")
head(geocru)
df96_23geo <- merge(geocru, df96_23, by="PROCED")
head(df96_23geo)
```


Luego, agrego las variables de esfuerzo y CPUE. Esto es otra forma de calcular el rendimiento, a traves de estos pasos. Si bien la base trae una columna ya calculada por fuera del esfuerzo y rendimiento, aqui hacemos el proceso demostrativo completo. Ambas columnas generadas CPUE y CPUE2 son iguales

```{r}
df1 <- df96_23geo

df1$a<-substr(df1$HOR_BUC, 1,1)
df1$b<-substr(df1$HOR_BUC, 2,2)
df1$c<-substr(df1$HOR_BUC, 3,3)
df1$d<-substr(df1$HOR_BUC, 4,4)

df1$e<-as.numeric(df1$a)
df1$f<-as.numeric(df1$b)
df1$g<-as.numeric(df1$c)
df1$h<-as.numeric(df1$d)

df1$Hr<-ifelse(df1$HOR_BUC<100,df1$HOR_BUC/60, df1$HOR_BUC)
df1$Hr<-ifelse(df1$HOR_BUC>99, (df1$f*10/60)+(df1$g/60)+df1$e, df1$Hr)
df1$Hr<-ifelse(df1$HOR_BUC>999, (df1$e*10)+df1$f+(df1$g*10/60)+(df1$h/60), df1$Hr)
names(df1)
df2<-df1[,c(1:28,37)]
names(df2)
```
Ahora procedo de los filtros de horas de buceo y numero de buzos. Es de consensos el hecho de quitar registros con numeros de buzos con NA o cero. Por otro lado, las horas de buceo se quitan nlos registros con 0 y no mayores a 10 horas, lo cual podria serr el maximo de una jornada de trabajo.

```{r}
# Elimino campos con buzos igual a 0 y bajo 5
df3<-df2 %>% 
  filter(N_BUZOS>0,
         N_BUZOS<6,
         Hr>0.0001,
         Hr<10) %>% 
  mutate(CPUE1 =CAPTURA_1/(Hr*N_BUZOS),
         CPUE2 =CAPTURA_2/(Hr*N_BUZOS)) %>% 
  filter(CPUE1<350,
         CPUE2<350)
         
dim(df3)
```

Generamos los Indices a evaluar aqui tengo tres opciones. Aquí transdformo algunas variables y a su vez calculo la CPUE como una nueva columna llamada `CPUE1`

```{r}
cpue1 <- ggplot(df3 %>%
                  filter(POLIGONO>0) %>% 
                  group_by(ANO_ARR,
                           POLIGONO) %>% 
                  summarize(MEAN=mean(CPUE1, na.rm=TRUE)),
                aes(x=ANO_ARR, y=MEAN))+
  geom_point(stat = 'identity', 
                   colour='#023e8a', 
                   fill='#023e8a', 
                   alpha=.5, size=2)+
  geom_smooth(method = "loess",
              span=0.4)+
  facet_wrap(.~POLIGONO)+
  theme_bw()+
  ylim(0, 200)
cpue1
```
El año 2023 no se consignó el datro de `CAPTURA_2`. Preguntar a CVicencio

Aqui podemos identificar el primer warning respecto al rendimiento pesquero global del erizo.

De aquí en mas debo trabajar con las variables `CAPTURA_1`y `CPUE1`dado que son las variables referidas a la captura en kilo que siempre hemos trabajado.

Se han utilizado tambien otras medidas de esfuerzo testeado durante el año 2017 en trabajo conjunto con Ana Parma

\pagebreak 

###  Cantidad de registros por variables

Registros por funcion

Existen varias funciones que estan relacionadas con el tipo de embarcacion, en este caso;

1: Acarradeora
E: Extractoras
M: Mixta
B:
L:

La función de la embarcacion en este aspecto es relevante, dado que las acarreadoras, si bien contienen gran cantidad de capturas, no se pueden rastrean de acuerdo a su origen. Para ello identificamos la cantidad de registros por funcion y tambien la magnnitud de capturas asociadas para cada función.

```{r}
table(df3$FUNCION)
```

Identifico los poligonos con mayor capturas en 2023

```{r}
t <- ggplot(df3 %>%  
  filter(ANO_ARR==2023) %>%            
  group_by(POLIGONO) %>% 
  summarise(total=sum(CAPTURA_1/1000)), aes(POLIGONO, total))+
  geom_bar(stat="identity", color=2, fill=2)+
  scale_x_continuous(breaks=0:13)+
  theme_bw()+
  ylab("Captura (t.) 2023")
t
```



\pagebreak 


A continuación genero un plot sencillo para ver cuan correlacionadas estan las variables. Check correlations (as scatterplots), distribution and print corrleation coefficient

```{r corre_var}
ggcorr(df3[,c(1,2,11, 12, 13, 25, 26, 28, 29, 30)],
        nbreaks = 4, 
       palette = "RdGy", 
       label = TRUE, 
       label_size = 3, 
       label_color = "white",
       hjust = 0.75)

```

De aqui en mas, trabajamos solo con la funcion E, que son las extractoras, dado que las lanchas Acarradoras (Funcion A) no tienen un monitoreo de procedencias, por lo cual no se los puede asignar una zona. A su vez mostramos las dimensiones de la base filtrada por función.

\pagebreak 

## Distribucion de variables

Saco los datos de CPUE sobre 350 que de acuerdo a la experiencia es una cantidad razonable de captura por accion de extracción

```{r}

erizo350 <- df3 
```

Ahora un grafico de distribución en el espacio de la Captura.


```{r echo=FALSE}
erizo350$LONGITUD <- as.numeric(as.character(erizo350$LONGITUD))
erizo350$LATITUD <- as.numeric(as.character(erizo350$LATITUD))
```


```{r}
#saco los datos geograficos dummies
erizo350<-erizo350[erizo350$LATITUD<0, ]

```
Hago un mapa con los shapes nuevos

```{r}
chi <- raster::getData("GADM", country = "CHL", level = 0)
chi@data
#
e <- extent(-77,-72,-46,-41)
#e2 <- extent(-70,-35,-68,-60)
rc <- crop(chi, e)
proj4string(rc) <- CRS("+init=epsg:4326")
rc3 <- st_as_sf(rc) # par dejarlo en formato geom_sf

# Saco bathymetria
bat <- getNOAA.bathy(-77,-72,-46,-41, res = 1)


#mapzona2 <- readShapePoly('~/DOCAS/Mapas/asd-shapefile-WGS84/asd-shapefile-WGS84.shp')

polifop <- st_read('/Users/mauriciomardones/IFOP/Erizo_SA/2022/Data_Seg_2021/poligono_ifop.shp')
polimo <- st_read('/Users/mauriciomardones/IFOP/Erizo_SA/2022/Data_Seg_2021/pol_51_mod.shp')

# debo cortar los limites de poliifop
# extent(box) <- c(-76, -71, -47, -41)
# polifopa <- st_crop(polifop, xlim = c(-76 , -71) , ylim = c(-41 , -47))
# 
# 
# dfSL <- PolySet2SpatialLines( df_sub )
```


Preparo la grilla para los diversos indicadores.

```{r}
Grid2<- rc3 %>% #rc3 es el plot base original linea 487
  sf::st_make_grid(cellsize = c(0.2,0.125)) %>% 
  sf::st_cast("MULTIPOLYGON") %>%
  sf::st_sf() %>% # objeto en spatial feature
  dplyr::mutate(cellid = row_number()) 

# la pruebo

ggplot() +
  geom_contour(data = bat,
               aes(x=x, y=y, z=z),
               breaks=c(0, -10, -20, -50, -100, -200, -1000),
               linewidth=0.2,
               colour="grey")+
  geom_sf(data = Grid2, 
          color="grey",  
          fill=NA, 
          size=0.3) +
  geom_sf(data = rc3,  fill=NA)+
  geom_sf(data = polimo, color="red",  fill=NA)+
  theme_minimal()

```

Cambio el formato y proyección de los datos crudos.

```{r}
#transformar los datos en un sf object. Aqui ojo por q no cambié el nombre de las tallas =LONGITUD
df3 <- st_as_sf(erizo350, coords = c(4, 3),  
                  crs = 4326)
# join data to grid; make sure to join points to grid
# the first object drives the output geometry
result4 <- Grid2 %>%
  st_join(df3) %>% 
  group_by(cellid, ANO_ARR) %>% 
  summarise(meancap = mean(CPUE1))%>% 
  filter(! is.na(meancap))

resultcap <- Grid2 %>%
  st_join(df3) %>% 
  group_by(cellid, ANO_ARR) %>% 
  summarise(meancatch = mean(CAPTURA_1))%>% 
  filter(! is.na(meancatch))
```

```{r fig.height=3}
cp1 <- ggplot() +
  # #geom_sf(data = polimo, color="red",  fill=NA)+
  # geom_contour(data = bat,
  #              aes(x=x, y=y, z=z),
  #              breaks=c(0, -10, -20, -50, -100, -200, -1000),
  #              linewidth=0.2,
  #              colour="grey")+
  geom_sf(data=result4 %>% 
            filter(ANO_ARR %in% c(1996, 
                                        2000,
                                        2005, 2010, 
                                        2015, 2020, 
                                        2023)), 
          aes(fill = cut(meancap,
                         breaks = seq(0, 1000, by =50))),
          color=NA) +
  scale_fill_brewer(labels =  seq(0, 300, by = 50), # if you must
                    palette = "Purples",
                    name = "CPUE erizo") +
  geom_sf(data = rc3, color="grey", 
          fill="white")+
  theme_few()+
  theme(axis.text.x = element_text(angle = 90, 
                                   hjust = 1,
                                   vjust=0,
                                   size = 6),
        axis.text.y = element_text(size = 6),
        legend.position = "bottom")+
  facet_wrap(.~ANO_ARR, ncol = 4)+
  coord_sf(crs = 4326)+
  labs(x = "Longitude", y = "Latitude")+
  ylim(-45.7, -41.3)+
  xlim(-75.2, -72.6)
cp1
```

```{r fig.cap="Distribucion CPUE Erizo 2023"}
cp <- ggplot() +
  geom_contour(data = bat,
               aes(x=x, y=y, z=z),
               breaks=c(0, -10, -20, -50, -100, -200, -1000),
               linewidth=0.2,
               colour="grey")+
  geom_sf(data=result4 %>% 
            filter(ANO_ARR %in% c(2023, 2022)), 
          aes(fill = cut(meancap,
                         breaks = seq(0, 1000, by = 50))),
          color=NA) +
  scale_fill_brewer(labels =  seq(0, 500, by = 50), # if you must
                    palette = "Purples",
                    name = "CPUE (kg/HrBuceo* NºBuzos)") +
  geom_sf(data = rc3, 
          color="black",  
          fill="white")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, 
                                   hjust = 1,
                                   vjust=0,
                                   size = 6),
        axis.text.y = element_text(size = 6),
        legend.position = "bottom")+
  facet_wrap(.~ANO_ARR, ncol = 2)+
  coord_sf(crs = 4326)+
  labs(x = "Longitude", y = "Latitude")+
  ylim(-45.7, -41.3)+
  xlim(-75.2, -72.6)
cp
```


```{r fig.height=3}
capt <- ggplot() +
  geom_sf(data=resultcap %>% 
            filter(ANO_ARR %in% c(1996, 
                                        2000,
                                        2005, 2010, 
                                        2015, 2020, 
                                        2023)),  
          aes(fill = cut(meancatch,
                         breaks = seq(500, 3000, by = 500))),
          color=NA) +
  scale_fill_brewer(labels =  seq(500, 3500, by = 500), # if you must
                    palette = "YlGn",
                    name = "Captura (t.)") +
  geom_sf(data = rc3, 
          color="grey",  
          fill="white")+
  theme_few()+
  theme(axis.text.x = element_text(angle = 90, 
                                   hjust = 1,
                                   vjust=0,
                                   size = 6),
        axis.text.y = element_text(size = 6),
        legend.position = "bottom")+
  facet_wrap(.~ANO_ARR, ncol = 4)+
  coord_sf(crs = 4326)+
  labs(x = "Longitude", y = "Latitude")+
  ylim(-45.7, -41.3)+
  xlim(-75.2, -72.6)
capt
```



Primer plot del comportamiento de la cpue  vs allgun factor como poligono o mes

calculo las medias de esfuerzo por sector


```{r BoxCpueMes}
#mes vs cpue
na <- ggplot(erizo350, aes(x=factor(ANO_ARR), y=Hr)) + 
  geom_boxplot()+
  facet_wrap(.~POLIGONO, ncol=7)+
  geom_hline(yintercept = 0.8, col="red")+
  geom_hline(yintercept = 2.1, col="blue")+
  theme_bw()+
  coord_flip()
na
```


Ahora se procede a generar df por cada una de las zonas de evaluacion. Estas zonas son llamadas como sigue;

1. Zona X Norte
2. Zona X Sur
3. Zona XI

```{r}
# Comandos para crear nueva columna de zona  en funcion de los poligonos

erizo350$ZONA[erizo350$POLIGONO %in% c(1,2)] <-1
erizo350$ZONA[erizo350$POLIGONO %in% c(4,5,6,13)] <-2
erizo350$ZONA[erizo350$POLIGONO %in% c(7,8,9,10,11,12)] <-3

erizo350$ZONA<- as.factor(erizo350$ZONA)
class(erizo350$ZONA)
```



```{r}
skimr::skim(erizo350)
```

Suma de las capturas por zona

```{r}
CATCHZONA<- aggregate(CAPTURA_1/1000~ ANO_ARR + ZONA,
                      data = erizo350, FUN = sum)
```



Ahora con puntos y un LOESS

```{r}
cpuem<- aggregate(CPUE2~ ANO_ARR + ZONA, data = erizo350, FUN = mean)

rt <- ggplot(cpuem %>% 
               drop_na(), aes(x=ANO_ARR, y=CPUE2)) + 
        geom_point(stat = 'identity', colour='#cb181d', fill='#cb181d', alpha=.9, size=2) +
        stat_smooth(method = "loess")+
        scale_x_continuous(breaks = seq(from = 1996, to = 2023, by = 3))+
        theme_few()+
        theme(axis.text.x = element_text(angle = 90, hjust = 2))+
        facet_wrap(.~ZONA, 
                   ncol =  3,
                   labeller = labeller(ZONA = c("1" = "X Norte", 
                                                 "2" = "X Sur",
                                                 "3" = "XI")))+
        ylab('CPUE (kg/hr*n Buzo)')+
        xlab('')+
        ylim(0,200)+
        ggtitle('')
rt
```




CPUE por Poligobnos Molinet

```{r}
cpuep<- aggregate(CPUE2~ ANO_ARR + POLIGONO+ ZONA, data = erizo350, FUN = mean)

rt2 <- ggplot(cpuep %>% 
               drop_na(), aes(x=ANO_ARR, y=CPUE2, colour=ZONA)) + 
        geom_point(stat = 'identity',  
                   fill='#006d2c',
                   alpha=.9, 
                   size=2) +
        stat_smooth(method = "loess")+
        #scale_x_continuous(breaks = seq(from = 1996, to = 2021, by = 1))+
        scale_color_brewer(palette = 10)+
        theme_bw()+
        theme(axis.text.x = element_text(angle = 90, hjust = 2))+
        facet_wrap(.~POLIGONO, ncol =  4)+
        ylab('CPUE (kg/hr*n Buzo')+
        xlab('')+
        ylim(0,250)+
        ggtitle('')
rt2
```
```{r}
effm<- aggregate(HOR_BUC~ ANO_ARR + ZONA, data = erizo350, FUN = mean)
eff <- ggplot(effm %>% 
               drop_na(), aes(x=ANO_ARR, y=HOR_BUC)) + 
        geom_point(stat = 'identity', 
                   colour="black", 
                   fill="black", 
                   alpha=.9, size=2) +
        stat_smooth(method = "loess",
                    col="black")+
        scale_x_continuous(breaks = seq(from = 1996, to = 2023, by = 3))+
        theme_few()+
        theme(axis.text.x = element_text(angle = 90, hjust = 2))+
        facet_wrap(.~ZONA, 
                   ncol =  3,
                   labeller = labeller(ZONA = c("1" = "X Norte", 
                                                 "2" = "X Sur",
                                                 "3" = "XI")))+
        ylab('Horas de Buceo')+
        xlab('')+
        ggtitle('')
eff
```

# Estandarizacion

```{r}
mod1 <- glm(CPUE2 ~ as.factor(ANO_ARR),
            data = erizo350)
env2 <- glm(CPUE2 ~ as.factor(ANO_ARR) + 
              as.factor(PROF),
            data = erizo350)
env3 <- glm(lepro ~ Year + 
              SIC +
             Chl ,
            data = envlen2r)
env4 <- glm(lepro ~ Year + 
              SIC +
              Chl + 
              SST ,
            data = envlen2r)
#spatial component
env5 <- glm(lepro ~ Year + 
              SIC +
              Chl +
              SST +
              cellid,
            data = envlen2r)
```


\pagebreak

## Creación de una base para la estandarización

Una vez concluido el analisis exploratorio y aplicacion de filtros de la base de bitacoras de capturas, procedemos a generar una base de uso para la estandarización del indice de abunmdancia para cada zona de evaluación del erizo del sur.

```{r eval=FALSE, echo=TRUE}
write.csv(cpuem, "datacpue_erizo_96_23.csv", sep=" ", row.names = F)

```



